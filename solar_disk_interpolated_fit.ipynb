{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing exotic packages:\n",
    "# %pip install pysolar exifread timezonefinder opencv-python tqdm suncalc scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pysolar.solar as solar\n",
    "import suncalc\n",
    "import datetime\n",
    "import exifread\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "import os\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\" # required to enable .exr for new opencv-python versions\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%config InlineBackend.print_figure_kwargs = {'facecolor' : \"w\"} # making matplotlib background white\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cgg.mff.cuni.cz/~tobias/sky_dataset.php\n",
    "\n",
    "### GPS coordinates of the shooting places\n",
    "\n",
    "the list of shootings (and metadata) is available in the Google Sheet \"[SkyGAN] picture shooting log\"\n",
    "\n",
    "- 2019-06-26_0848_prague_chodska_21_rooftop: there is an anomaly at the end - maybe just invalid file timestamps?\n",
    "- 2019-07-15_1929_prague_chodska_21_rooftop: the Sun goes down way faster than expected - maybe incorrect time on the camera? or bad zoom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_dict = {\n",
    "    #'place_or_shooting_name': (latitude, longitude),\n",
    "    'prague_chodska_21': (50.074958, 14.445574),\n",
    "    '2019-05-24_1451_divoka_sarka': (50.09649, 14.321870),\n",
    "    '2019-06-04_1010_ronan': (50.0883, 14.4481),\n",
    "    '2019-06-04_1117_ronan': (50.0999, 14.4640),\n",
    "    '2019-06-04_1659_ronan': (50.0964, 14.3203),\n",
    "    '2019-06-05_1215_ronan': (50.0869, 14.4518),\n",
    "    '2019-06-05_1700_ronan': (50.0964, 14.3203),\n",
    "    '2019-06-06_1158_ronan': (50.0891, 14.4524),\n",
    "    '2019-06-06_1257_ronan': (50.0851, 14.4602),\n",
    "    '_hammerschmiede': (48.944980, 9.972590),\n",
    "    '2019-06-08_1545_quarry': (49.292050, 9.808800),\n",
    "    'farm_vystice': (49.122424, 14.390338),\n",
    "    '2019-06-29_1113_divoka_sarka': (50.1006875, 14.3222442),\n",
    "    '2019-07-04_1749_lake_lhota': (50.24481, 14.66734),\n",
    "    'santa_cruz_villa_nuova': (36.976024, -122.020524),\n",
    "    'burbank_1200_riverside': (34.158442, -118.312839),\n",
    "    'centrum_nesmen_by_te_wood_logs':  (48.8095625, 14.5562914),\n",
    "    'centrum_nesmen_by_the_wood_logs': (48.8095625, 14.5562914),\n",
    "    'centrum_nesmen_hilltop': (48.8131133, 14.5541778),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-time retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_from_exif(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        tags = exifread.process_file(f)\n",
    "        dt = tags['EXIF DateTimeOriginal'].values\n",
    "        return datetime.datetime.strptime(dt, \"%Y:%m:%d %H:%M:%S\")\n",
    "        # NOTE: EXIF's time zone implementation is only quite recent and not supported here\n",
    "\n",
    "def get_latlong_from_shooting_name(datetime_and_place):\n",
    "    matched = None # (place_name, (lat, long))\n",
    "    for place_name, pos in places_dict.items():\n",
    "        if datetime_and_place.__contains__(place_name):\n",
    "            if matched is not None:\n",
    "                raise Exception('multiple places matched: {} and {}'.format(matched[0], place_name))\n",
    "            matched = (place_name, pos)\n",
    "    if matched is None:\n",
    "        raise Exception('position not found based on directory name: add it to \"places_dict\"')\n",
    "    return matched[1]\n",
    "\n",
    "def get_latlong(fname):\n",
    "    assert(fname[-3:] == 'CR2')\n",
    "    datetime_and_place = fname.split('/')[-2]\n",
    "    # TODO assert format \"YYYY-MM-DD_HHMM_placename\"\n",
    "    return get_latlong_from_shooting_name(datetime_and_place)\n",
    "\n",
    "def get_datetime(fname, tzf): #Â this one \n",
    "    latitude, longitude = get_latlong(fname)\n",
    "    t = tzf.timezone_at(lng=longitude, lat=latitude)\n",
    "    tz = pytz.timezone(t)\n",
    "    dt = get_datetime_from_exif(fname)\n",
    "    return tz.localize(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sun coordinates from location and date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elevation_azimuth(latitude, longitude, date):\n",
    "    result = suncalc.get_position(date.astimezone(datetime.timezone.utc),\n",
    "        longitude, latitude)\n",
    "    return np.rad2deg(result[\"altitude\"]), np.rad2deg(result[\"azimuth\"]) + 180\n",
    "    # return (solar.get_altitude(latitude, longitude, date), # solar.get_altitude(): calculate the angle between the sun and a plane tangent to the earth where you are. The result is returned in degrees.\n",
    "    #         solar.get_azimuth(latitude, longitude, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sun coordinates from an image (searching for a solar disk in an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elevation_azimuth_from_fisheye_xy(xy, radius):\n",
    "    x, y = xy\n",
    "\n",
    "    def dist_sqr(x1, x2, y1, y2):\n",
    "        return (x1-x2)*(x1-x2) + (y1-y2)*(y1-y2)\n",
    "\n",
    "    center = radius\n",
    "\n",
    "    if dist_sqr(center, x, center, y) >= radius * radius: # is inside the dome-circle?\n",
    "        # raise Exception('Querying a fisheye coordinate outside of the dome-circle')\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    x_rel = (x - center) / radius\n",
    "    y_rel = (y - center) / radius\n",
    "\n",
    "    # print(x_rel, y_rel)\n",
    "\n",
    "    xy_rel_len = math.sqrt(x_rel*x_rel + y_rel*y_rel)\n",
    "\n",
    "    x_rel_norm = x_rel / xy_rel_len\n",
    "    y_rel_norm = y_rel / xy_rel_len\n",
    "\n",
    "    azimuth_rad = math.atan2(y_rel_norm, x_rel_norm)\n",
    "    if azimuth_rad < math.pi:\n",
    "        azimuth_rad += 2 * math.pi\n",
    "    \n",
    "    elevation_rad = 2 * math.atan(1/xy_rel_len) - math.pi / 2.0\n",
    "\n",
    "    azimuth_picture_deg = azimuth_rad * 180 / math.pi\n",
    "    elevation_deg = elevation_rad * 180 / math.pi\n",
    "\n",
    "    # conversion between real-world azimuth and our visualization (fisheye photos of skydome model ... north is up, west is to the right)\n",
    "    azimuth_realworld_deg = 270 - azimuth_picture_deg\n",
    "    if azimuth_realworld_deg < 0:\n",
    "        azimuth_realworld_deg += 360\n",
    "        \n",
    "    return elevation_deg, azimuth_realworld_deg\n",
    "\n",
    "# def get_elevation_azimuth_from_fisheye_xy(xy_imagecoord, radius):\n",
    "#     x_imagecoord, y_imagecoord = xy_imagecoord\n",
    "\n",
    "#     def dist_sqr(x1, x2, y1, y2):\n",
    "#         return (x1-x2)*(x1-x2) + (y1-y2)*(y1-y2)\n",
    "\n",
    "#     # if x_imagecoord * x_imagecoord + y_imagecoord * y_imagecoord > radius * radius: # is inside the dome-circle?\n",
    "#     #     raise Exception('Querying a fisheye coordinate outside of the dome-circle')\n",
    "\n",
    "#     x = (x_imagecoord - radius) / radius\n",
    "#     y = (y_imagecoord - radius) / radius\n",
    "\n",
    "#     print(x, y)\n",
    "\n",
    "#     if x * x + y * y > 1.0: # is inside the dome-circle?\n",
    "#         raise Exception('Querying a fisheye coordinate outside of the dome-circle')\n",
    "\n",
    "#     z = np.sqrt(1.0 - x * x - y * y) # assuming the upper hemisphere\n",
    "\n",
    "#     elevation = np.pi / 2.0 - np.arccos(z)\n",
    "#     azimuth = np.arctan2(y, x) + np.pi\n",
    "        \n",
    "#     return (np.rad2deg(elevation), np.rad2deg(azimuth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_solar_disk_center(image):\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "#     mask_threshold = 0.5\n",
    "#     image_mask = (image >= mask_threshold).astype(float) # 1 = bright pixel, 0 = not bright pixel\n",
    "#     # plt.imshow(image_mask)\n",
    "#     image_mask_nonzero = np.transpose(image_mask.nonzero())\n",
    "#     if len(image_mask_nonzero) > 0:\n",
    "#         center = np.mean(np.transpose(image_mask.nonzero()), axis=0)[::-1]\n",
    "#     else:\n",
    "#         center = [0,0]\n",
    "#     # plt.scatter(center[0], center[1], marker='x')\n",
    "#     # plt.xlim(500,700)\n",
    "#     # plt.ylim(600,800)\n",
    "#     return center\n",
    "\n",
    "def find_solar_disk_center(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    mask_threshold = 0.5\n",
    "    _, image_mask = cv2.threshold(image, mask_threshold, 255, cv2.THRESH_BINARY)\n",
    "    image_mask = np.array(image_mask).astype(np.uint8)\n",
    "    plt.imshow(image_mask)\n",
    "    plt.show()\n",
    "\n",
    "    #im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_FLOODFILL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours, hierarchy = cv2.findContours(image_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    #contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    #print(contours)\n",
    "    #print(hierarchy.shape)\n",
    "    # num_contours = hierarchy.shape[1]\n",
    "    if hierarchy is None or hierarchy.shape[1] != 1:\n",
    "        return [0,0] # Multiple bright objects found\n",
    "    \n",
    "    #print(len(contours[0]))\n",
    "    if len(contours[0]) < 5:\n",
    "        return [0,0] # Too few points - probably not an ellipse\n",
    "\n",
    "    ellipse = cv2.fitEllipse(contours[0])\n",
    "\n",
    "    return ellipse[0] # ellipse in the format (center x, center y), (width, height), angle in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "\n",
    "path = \"/projects/SkyGAN/clouds_fisheye/processed/2019-06-24_0504_prague_chodska_21_rooftop/1K_EXR/IMG_9864_hdr.exr\"\n",
    "path = \"/projects/SkyGAN/clouds_fisheye/processed/2019-07-14_1418_prague_chodska_21_rooftop/1K_EXR/IMG_3596_hdr.exr\"\n",
    "image = cv2.imread(path, flags = cv2.IMREAD_ANYDEPTH | cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "solar_disk_center = find_solar_disk_center(image)\n",
    "get_elevation_azimuth_from_fisheye_xy(solar_disk_center[::-1], np.shape(image)[0] / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of .exr files, it checks their timestamps and tries to detect the solar disk positions in the files\n",
    "def process_subset(paths_exr):\n",
    "    tzf = TimezoneFinder() # needs to exist per each thread separately!\n",
    "    results = {\n",
    "        \"t\": [],\n",
    "        \"lat\": [],\n",
    "        \"lon\": [],\n",
    "        \"detected_azimuth\": [],\n",
    "        \"detected_elevation\": [],\n",
    "        \"img_fname\": [],\n",
    "    }\n",
    "    for path_exr in paths_exr:\n",
    "\n",
    "        path_raw = Path(str(path_exr).replace('/processed/', '/raw/').replace('/1K_EXR', '').replace('_hdr.exr', '.CR2'))\n",
    "        assert path_raw.exists()\n",
    "\n",
    "        results[\"img_fname\"].append(str(path_exr))\n",
    "\n",
    "        latitude, longitude = get_latlong(str(path_raw))\n",
    "        datetime_ = get_datetime(str(path_raw), tzf)\n",
    "        results[\"t\"].append(datetime_)\n",
    "        results[\"lat\"].append(latitude)\n",
    "        results[\"lon\"].append(longitude)\n",
    "\n",
    "        image = cv2.imread(str(path_exr), flags = cv2.IMREAD_ANYDEPTH | cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        detected_solar_disk_center = find_solar_disk_center(image)\n",
    "        detected_elevation, detected_azimuth = get_elevation_azimuth_from_fisheye_xy(\n",
    "            detected_solar_disk_center, np.shape(image)[0] / 2.0)\n",
    "        results[\"detected_elevation\"].append(detected_elevation)\n",
    "        results[\"detected_azimuth\"].append(detected_azimuth)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotEnoughDataException(Exception):\n",
    "    pass\n",
    "\n",
    "# Given a path to a dataset (a directory containing many .exr files), it parallelizes calls to process_subset\n",
    "# to detect the solar disk positions in all the images, and it tries to find a time offset in which these\n",
    "# images were probably taken, given the difference between the expected sun disk positions and the detected\n",
    "# sun disk positions from the images\n",
    "def process_dataset(path):\n",
    "    path = Path(path)\n",
    "\n",
    "    def chunks(lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    results = {\n",
    "        \"t\": [],\n",
    "        \"lat\": [],\n",
    "        \"lon\": [],\n",
    "        \"detected_azimuth\": [],\n",
    "        \"detected_elevation\": [],\n",
    "        \"img_fname\": [],\n",
    "    }\n",
    "    paths_exr = list(sorted(path.glob(\"*_hdr.exr\")))#[::10]\n",
    "\n",
    "    if len(paths_exr) < 50:\n",
    "        raise NotEnoughDataException(\"There is less than 50 photos in the dataset, which is not enough to meaningfully fit the solar disk positions\")\n",
    "\n",
    "    print(\"Starting processing the dataset\")\n",
    "\n",
    "    # Parallel processing:\n",
    "    pool_size = 8\n",
    "    chunk_size = 20\n",
    "    pool = mp.Pool(pool_size)\n",
    "    paths_exr_chunked = list(chunks(paths_exr, chunk_size))\n",
    "    with tqdm(total=len(paths_exr)) as progress_bar:\n",
    "        for results_ in pool.imap_unordered(process_subset, paths_exr_chunked):\n",
    "            progress_bar.update(len(results_[\"t\"]))\n",
    "            for key, value in results.items():\n",
    "                value.extend(results_[key])\n",
    "\n",
    "    # # Non-parallel processing:\n",
    "    # results = process_subset(paths_exr)\n",
    "\n",
    "    if np.count_nonzero(~np.isnan(results[\"detected_azimuth\"])) < 20:\n",
    "        raise NotEnoughDataException(\"Less than 20 solar disk positions were successfully detected, which is not enough to meaningfully fit the solar disk positions\")\n",
    "\n",
    "    # Sorting the results by time:\n",
    "    print(\"Sorting the results by time\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by=\"t\", inplace=True)\n",
    "    results = results_df.to_dict(\"list\")\n",
    "\n",
    "    expected_elevation_azimuth = np.array([\n",
    "        compute_elevation_azimuth(lat, lon, dt) for lat, lon, dt in zip(results[\"lat\"], results[\"lon\"], results[\"t\"])\n",
    "    ])\n",
    "    results[\"expected_elevation\"] = expected_elevation_azimuth[:,0]\n",
    "    results[\"expected_azimuth\"] = expected_elevation_azimuth[:,1]\n",
    "\n",
    "    # Finding the time offset:\n",
    "    print(\"Finding the time offset\")\n",
    "\n",
    "    def find_time_offset():\n",
    "        fig, axs = plt.subplots(nrows=2, tight_layout=True, figsize=(12,8))\n",
    "\n",
    "        ax = axs[0]\n",
    "        ax.set_title(\"Azimuth error and offset\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M', tz=results[\"t\"][0].tzinfo))\n",
    "        ax = axs[1]\n",
    "        ax.set_title(\"Elevation error and offset\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M', tz=results[\"t\"][0].tzinfo))\n",
    "\n",
    "        best_error = np.inf\n",
    "        best_time_offset_seconds = 0\n",
    "        best_elevation_azimuth_offset = (0, 0)\n",
    "        best_corrected_elevation_azimuth = []\n",
    "        for time_offset_seconds in np.arange(-20*60, 20*60+15, 15, dtype=float):\n",
    "            corrected_elevation_azimuth = np.array([\n",
    "                compute_elevation_azimuth(lat, lon, dt + datetime.timedelta(seconds = time_offset_seconds)) for lat, lon, dt in zip(results[\"lat\"], results[\"lon\"], results[\"t\"])\n",
    "            ])\n",
    "            error_elevation = np.array(results[\"detected_elevation\"]) - corrected_elevation_azimuth[:,0]\n",
    "            error_azimuth = np.array(results[\"detected_azimuth\"]) - corrected_elevation_azimuth[:,1]\n",
    "            elevation_azimuth_offset = (np.nanmedian(error_elevation), np.nanmedian(error_azimuth))\n",
    "            corrected_elevation_azimuth[:,0] += elevation_azimuth_offset[0]\n",
    "            corrected_elevation_azimuth[:,1] += elevation_azimuth_offset[1]\n",
    "            error_elevation = np.array(results[\"detected_elevation\"]) - corrected_elevation_azimuth[:,0]\n",
    "            error_azimuth = np.array(results[\"detected_azimuth\"]) - corrected_elevation_azimuth[:,1]\n",
    "            error_elevation_0d = np.nanmedian(np.abs(error_elevation))\n",
    "            error_azimuth_0d = np.nanmedian(np.abs(error_azimuth))\n",
    "            error = error_elevation_0d + error_azimuth_0d\n",
    "            ax = axs[0]\n",
    "            ax.scatter(results[\"t\"], error_azimuth, label=f\"T={time_offset_seconds:.0f}s, Err={error_azimuth_0d:.3f}\", s=3)\n",
    "            ax = axs[1]\n",
    "            ax.scatter(results[\"t\"], error_elevation, label=f\"T={time_offset_seconds:.0f}s, Err={error_elevation_0d:.3f}\", s=3)\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_time_offset_seconds = time_offset_seconds\n",
    "                best_elevation_azimuth_offset = elevation_azimuth_offset\n",
    "                best_corrected_elevation_azimuth = corrected_elevation_azimuth\n",
    "        \n",
    "        def plot_best_results():\n",
    "            ax = axs[0]\n",
    "            ax.plot(results[\"t\"], np.array(results[\"detected_azimuth\"]) - best_corrected_elevation_azimuth[:,1], label=f\"Best\", color=\"red\")\n",
    "            ax = axs[1]\n",
    "            ax.plot(results[\"t\"], np.array(results[\"detected_elevation\"]) - best_corrected_elevation_azimuth[:,0], label=f\"Best\", color=\"red\")\n",
    "        plot_best_results()\n",
    "\n",
    "        def plot_zeros():\n",
    "            ax = axs[0]\n",
    "            ax.axhline(y = 0, color=\"black\", linestyle=\"--\")\n",
    "            ax = axs[1]\n",
    "            ax.axhline(y = 0, color=\"black\", linestyle=\"--\")\n",
    "        plot_zeros()\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.set_xlim(sorted(results[\"t\"])[0], sorted(results[\"t\"])[-1])\n",
    "            # ax.legend()\n",
    "            ax.grid()\n",
    "\n",
    "        return best_time_offset_seconds, best_elevation_azimuth_offset, best_corrected_elevation_azimuth\n",
    "\n",
    "    time_offset, elevation_azimuth_offset, corrected_elevation_azimuth = find_time_offset()\n",
    "\n",
    "    print(\"Corrected time offset:\", time_offset)\n",
    "\n",
    "    results[\"corrected_elevation\"] = corrected_elevation_azimuth[:,0]\n",
    "    results[\"corrected_azimuth\"] = corrected_elevation_azimuth[:,1]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual \"main\" cell that runs the script for the selected dataset paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_names = '''2019-05-10_1755_podbaba\n",
    "2019-05-17_1946_hodejovice\n",
    "2019-05-18_1357_hodejovice\n",
    "2019-05-24_1451_divoka_sarka\n",
    "2019-06-04_1010_ronan\n",
    "2019-06-04_1117_ronan\n",
    "2019-06-04_1659_ronan\n",
    "#2019-06-05_1215_ronan - the camera moves while shooting\n",
    "2019-06-05_1700_ronan\n",
    "2019-06-06_1158_ronan\n",
    "2019-06-06_1257_ronan\n",
    "2019-06-07_1306_hammerschmiede\n",
    "2019-06-08_1545_quarry\n",
    "2019-06-10_1235_hammerschmiede\n",
    "2019-06-12_1744_stromovka_cgbbq\n",
    "2019-06-16_1906_prague_chodska_21_rooftop\n",
    "2019-06-18_0523_prague_chodska_21_rooftop\n",
    "#2019-06-20_0454_prague_chodska_21_rooftop - exr vs. raw mismatch!\n",
    "2019-06-20_0851_prague_chodska_21_rooftop\n",
    "2019-06-20_1151_prague_chodska_21_rooftop\n",
    "#2019-06-21_0825_prague_chodska_21_rooftop - some mixup? the shoot is from the morning, but the Sun is going down???\n",
    "2019-06-22_1859_farm_vystice\n",
    "2019-06-23_0943_farm_vystice\n",
    "2019-06-23_1939_prague_chodska_21_rooftop\n",
    "#2019-06-24_0504_prague_chodska_21_rooftop - exr vs. raw mismatch\n",
    "2019-06-24_0827_prague_chodska_21_rooftop\n",
    "2019-06-25_1952_prague_chodska_21_rooftop\n",
    "2019-06-26_0848_prague_chodska_21_rooftop\n",
    "2019-06-26_1311_prague_chodska_21_rooftop\n",
    "2019-06-26_1855_prague_chodska_21_rooftop\n",
    "2019-06-29_1113_divoka_sarka\n",
    "2019-07-04_1749_lake_lhota\n",
    "2019-07-05_0812_prague_chodska_21_rooftop\n",
    "#2019-07-07_2132_prague_chodska_21_rooftop - night, fits to the moon instead of the sun\n",
    "2019-07-13_1239_prague_chodska_21_rooftop\n",
    "2019-07-13_1524_prague_chodska_21_rooftop\n",
    "2019-07-13_1905_prague_chodska_21_rooftop\n",
    "2019-07-14_1054_prague_chodska_21_rooftop\n",
    "2019-07-14_1317_prague_chodska_21_rooftop\n",
    "2019-07-14_1418_prague_chodska_21_rooftop\n",
    "2019-07-14_1603_prague_chodska_21_rooftop\n",
    "2019-07-14_1618_prague_chodska_21_rooftop\n",
    "2019-07-15_0844_prague_chodska_21_rooftop\n",
    "2019-07-15_1929_prague_chodska_21_rooftop\n",
    "2019-07-16_0845_prague_chodska_21_rooftop\n",
    "2019-07-16_1221_prague_chodska_21_rooftop\n",
    "2019-07-17_0547_prague_chodska_21_rooftop\n",
    "2019-07-17_0856_prague_chodska_21_rooftop\n",
    "2019-07-17_1732_prague_chodska_21_rooftop\n",
    "2019-07-17_1957_prague_chodska_21_rooftop\n",
    "2019-08-05_0841_santa_cruz_villa_nuova\n",
    "#2019-08-05_1716_santa_cruz_villa_nuova - poor results\n",
    "2019-08-05_2013_santa_cruz_villa_nuova\n",
    "2019-08-06_0933_santa_cruz_villa_nuova\n",
    "2019-08-06_1657_santa_cruz_villa_nuova\n",
    "2019-08-08_1048_santa_cruz_villa_nuova\n",
    "2019-08-09_0800_santa_cruz_villa_nuova\n",
    "2019-08-09_1800_santa_cruz_villa_nuova\n",
    "2019-08-10_1000_santa_cruz_villa_nuova\n",
    "2019-08-10_1959_santa_cruz_villa_nuova\n",
    "2019-08-16_1130_burbank_1200_riverside\n",
    "2019-08-16_1824_burbank_1200_riverside'''.split('\\n')\n",
    "\n",
    "for i, shooting_name in enumerate(shooting_names):\n",
    "    if shooting_name[0] == \"#\":\n",
    "        print(f\"{i:03d}/{len(shooting_names):03d} Skipping dataset\", shooting_name)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        path = Path(\"/projects/SkyGAN/clouds_fisheye/processed/\") / shooting_name / \"1K_EXR\"\n",
    "        output_path = Path(\"/projects/SkyGAN/clouds_fisheye/solar_disk_fitting/\") / shooting_name\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"{i:03d}/{len(shooting_names):03d} Processing dataset\", path)\n",
    "\n",
    "        results = process_dataset(path)\n",
    "        plt.savefig(str(output_path / \"time_offset_fitting.png\"))\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=2, tight_layout=True, figsize=(12,8))\n",
    "        ax = axs[0]\n",
    "        ax.set_title(\"Azimuth\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M', tz=results[\"t\"][0].tzinfo))\n",
    "        ax.plot(results[\"t\"], results[\"expected_azimuth\"], label=\"Expected (from date, time, and location)\", zorder=1, linewidth=3)\n",
    "        ax.scatter(results[\"t\"], results[\"detected_azimuth\"], label=\"Detected (from image)\", zorder=3, s=10, color=\"C1\")\n",
    "        ax.plot(results[\"t\"], results[\"corrected_azimuth\"], label=\"Corrected\", zorder=2, linewidth=3, color=\"C2\")\n",
    "        # ax.set_ylim(60,70)\n",
    "        # y_fit = fit_curve([dt.timestamp() for dt in results[\"t\"]], results[\"detected_azimuth\"])\n",
    "        # ax.plot(results[\"t\"], y_fit, color=\"C1\")\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "        ax = axs[1]\n",
    "        ax.set_title(\"Elevation\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M', tz=results[\"t\"][0].tzinfo))\n",
    "        ax.plot(results[\"t\"], results[\"expected_elevation\"], label=\"Expected (from date, time, and location)\", zorder=1, linewidth=3)\n",
    "        ax.scatter(results[\"t\"], results[\"detected_elevation\"], label=\"Detected (from image)\", zorder=3, s=10, color=\"C1\")\n",
    "        ax.plot(results[\"t\"], results[\"corrected_elevation\"], label=\"Corrected\", zorder=2, linewidth=3, color=\"C2\")\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "\n",
    "        plt.savefig(str(output_path / \"solar_disk_fitting.png\"))\n",
    "        plt.show()\n",
    "\n",
    "        np.savez_compressed(str(output_path / \"fitting_results.npz\"), **results)\n",
    "    except NotEnoughDataException as e:\n",
    "        print(\"!!! !!! !!!\")\n",
    "        print(f\"Skipping dataset {shooting_name}\")\n",
    "        print(e)\n",
    "        print(\"!!! !!! !!!\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"!!! !!! !!!\")\n",
    "        print(f\"Exception caught, dataset {shooting_name} could not be processed\")\n",
    "        # print(e)\n",
    "        traceback.print_exc()\n",
    "        print(\"!!! !!! !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
